{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f18f60f",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "818d8ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !jupyter kernelspec uninstall w210_env --y\n",
    "\n",
    "# https://medium.com/@nrk25693/how-to-add-your-conda-environment-to-your-jupyter-notebook-in-just-4-steps-abeab8b8d084"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "074b5d1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/ec2-user/SageMaker/teledermatologyAI_capstone/notebooks/modelling'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f92cd16c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/SageMaker/teledermatologyAI_capstone\n"
     ]
    }
   ],
   "source": [
    "cd /home/ec2-user/SageMaker/teledermatologyAI_capstone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d869438a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new virtual env\n",
    "# !python -m venv w210_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee395a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this for every instance spin up\n",
    "# !source w210_env/bin/activate && ipython kernel install --user --name w210_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06afc59c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this if added new packages\n",
    "# !./w210_env/bin/pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f71a55ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -r requirements.txt\n",
    "# !pip uninstall sklearn --y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265df680",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# print(sys.prefix)\n",
    "# !which pip\n",
    "# !./w210_env/bin/pip show scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "151995a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import shutil\n",
    "import time\n",
    "import copy\n",
    "import random\n",
    "import itertools\n",
    "from datetime import datetime\n",
    "from pytz import timezone\n",
    "\n",
    "import pandas as pd\n",
    "from pandas_profiling import ProfileReport\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mp_image\n",
    "import seaborn as sns\n",
    "\n",
    "from IPython.display import Image, display\n",
    "\n",
    "from torchvision.datasets import ImageFolder\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.mobile_optimizer import optimize_for_mobile\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch_lr_finder import LRFinder\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "\n",
    "import awswrangler as wr\n",
    "\n",
    "from pytorchtools import EarlyStopping\n",
    "\n",
    "# expand pandas df rows/column widths etc.\n",
    "pd.set_option(\"display.max_rows\", None, # display all rows\n",
    "              \"display.max_columns\", None, # display all columns\n",
    "              \"display.max_colwidth\", None, # expand column width\n",
    "              \"display.html.use_mathjax\", False\n",
    "             ) # disable Latex style mathjax rendering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e64d4b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data split: data_class_folder3\n",
      "Total classes: 5\n",
      "Total num train images: 13500\n",
      "Total num val images: 2700\n",
      "Total num test images: 1800\n",
      "                                                Class  Train  Val  Test\n",
      "0                              Benign Marking or Mole   2707  542   351\n",
      "1                        Non-Cancerous Skin Condition   2671  534   395\n",
      "2                   Potentially Malignant Skin Tumors   2710  542   348\n",
      "3  Toxin, Fungal, Bug, Viral, or Bacterial Infections   2702  540   358\n",
      "4                                        Unclassified   2710  542   348\n"
     ]
    }
   ],
   "source": [
    "# credits: https://github.com/yuliyabohdan/Skin-diseases-classification-Dermnet-/blob/main/skin_diseases_clas_ResNet50.ipynb\n",
    "data_split = 'split_3'\n",
    "data_dir = 'data_class_folder3'\n",
    "\n",
    "DIR = data_dir\n",
    "DIR_TRAIN = f'{DIR}/train/'\n",
    "DIR_VAL = f'{DIR}/val/'\n",
    "DIR_TEST = f'{DIR}/test/' \n",
    "\n",
    "classes = sorted(os.listdir(DIR_TRAIN))\n",
    "print(f'Data split: {DIR}')\n",
    "print(f'Total classes: {len(classes)}')\n",
    "\n",
    "# total train, val and test images\n",
    "train_count = 0\n",
    "val_count = 0\n",
    "test_count = 0\n",
    "\n",
    "classes_df = []\n",
    "for _class in classes:\n",
    "    class_dict = {}\n",
    "    train_count += len(os.listdir(DIR_TRAIN + _class))\n",
    "    val_count += len(os.listdir(DIR_VAL + _class))\n",
    "    test_count += len(os.listdir(DIR_TEST + _class))\n",
    "    class_dict.update({'Class': _class, \n",
    "                       'Train': len(os.listdir(DIR_TRAIN + _class)),\n",
    "                       'Val': len(os.listdir(DIR_VAL + _class)),\n",
    "                       'Test': len(os.listdir(DIR_TEST + _class)) })\n",
    "    classes_df.append(class_dict)\n",
    "\n",
    "print(f'Total num train images: {train_count}')\n",
    "print(f'Total num val images: {val_count}')\n",
    "print(f'Total num test images: {test_count}')\n",
    "print(pd.DataFrame(classes_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6650603e",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e27852e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_parameter_requires_grad(model, feature_extracting):\n",
    "    if feature_extracting:\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False\n",
    "            \n",
    "def initialize_model(model_name, num_classes, feature_extract=True):\n",
    "    # Initialize these variables which will be set in this if statement. Each of these\n",
    "    #   variables is model specific.\n",
    "    model_ft = None\n",
    "    input_size = 0\n",
    "\n",
    "    if model_name == \"resnet\":\n",
    "        \"\"\" Resnet50\n",
    "        \"\"\"\n",
    "        model_ft = models.resnet50(weights='DEFAULT')\n",
    "        #we can select any possible variation of ResNet such as Resnet18, Resnet34, Resnet50, Resnet101, and Resnet152\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.fc.in_features\n",
    "        model_ft.fc = nn.Linear(num_ftrs, num_classes)\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"alexnet\":\n",
    "        \"\"\" Alexnet\n",
    "        \"\"\"\n",
    "        model_ft = models.alexnet(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.classifier[6].in_features\n",
    "        model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"vgg\":\n",
    "        \"\"\" VGG11_bn\n",
    "        \"\"\"\n",
    "        model_ft = models.vgg11_bn(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.classifier[6].in_features\n",
    "        model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"squeezenet\":\n",
    "        \"\"\" Squeezenet\n",
    "        \"\"\"\n",
    "        model_ft = models.squeezenet1_0(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        model_ft.classifier[1] = nn.Conv2d(512, num_classes, kernel_size=(1,1), stride=(1,1))\n",
    "        model_ft.num_classes = num_classes\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"densenet\":\n",
    "        \"\"\" Densenet\n",
    "        \"\"\"\n",
    "        model_ft = models.densenet121(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.classifier.in_features\n",
    "        model_ft.classifier = nn.Linear(num_ftrs, num_classes) \n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"inception\":\n",
    "        \"\"\" Inception v3 \n",
    "        Be careful, expects (299,299) sized images and has auxiliary output\n",
    "        \"\"\"\n",
    "        model_ft = models.inception_v3(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        # Handle the auxilary net\n",
    "        num_ftrs = model_ft.AuxLogits.fc.in_features\n",
    "        model_ft.AuxLogits.fc = nn.Linear(num_ftrs, num_classes)\n",
    "        # Handle the primary net\n",
    "        num_ftrs = model_ft.fc.in_features\n",
    "        model_ft.fc = nn.Linear(num_ftrs,num_classes)\n",
    "        input_size = 299\n",
    "\n",
    "    else:\n",
    "        print(\"Invalid model name, exiting...\")\n",
    "        exit()\n",
    "\n",
    "    return model_ft, input_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "368630ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/67959327/how-to-calculate-the-f1-score\n",
    "def train_model(model, dataloaders, criterion, optimizer, num_epochs, patience):\n",
    "    since = time.time()\n",
    "\n",
    "    val_acc_history = []\n",
    "    val_loss_history = []\n",
    "    train_acc_history = []\n",
    "    train_loss_history = []\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "    \n",
    "    # initialize the early_stopping object\n",
    "    early_stopping = EarlyStopping(patience=patience, verbose=True)\n",
    "\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('\\nEpoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:               \n",
    "                model.eval()   # Set model to evaluate mode\n",
    "                #update_bn_stats(model=model, data_loader=dataloaders[phase])  # if update_bn_stats\n",
    "                \n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    # Get model outputs and calculate loss\n",
    "                    # Special case for inception because in training it has an auxiliary output. In train\n",
    "                    #   mode we calculate the loss by summing the final output and the auxiliary output\n",
    "                    #   but in testing we only consider the final output.\n",
    "                    \n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                      # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "                f1score = f1_score(labels.cpu().data, preds.cpu(), average='macro')\n",
    "\n",
    "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
    "            \n",
    "            print('{} Loss: {:.4f} Acc: {:.4f} f1_score: {:.4f}'.format(phase, epoch_loss, epoch_acc, f1score))\n",
    "            \n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            if phase == 'val':\n",
    "                val_acc_history.append(epoch_acc)\n",
    "                val_loss_history.append(epoch_loss)\n",
    "                \n",
    "#                 # early_stopping needs the validation loss to check if it has decresed, \n",
    "#                 # and if it has, it will make a checkpoint of the current model\n",
    "#                 early_stopping(epoch_loss, model)\n",
    "#                 if early_stopping.early_stop:\n",
    "#                     best_acc = epoch_acc\n",
    "#                     best_model_wts = copy.deepcopy(model.state_dict())\n",
    "#                     print(\"Early stopping\")\n",
    "#                     break     \n",
    "            if phase == 'train':\n",
    "                train_acc_history.append(epoch_acc)\n",
    "                train_loss_history.append(epoch_loss)\n",
    "        \n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "   \n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, val_acc_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "654cd1fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, dl, normalize=True):\n",
    "    model.eval()\n",
    "    true_labels = []\n",
    "    predictions = []\n",
    "    total = 0\n",
    "    num_correct = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in dl:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            predicted = torch.argmax(outputs.data,-1)        \n",
    "            true_labels.append(labels.cpu().numpy())\n",
    "            predictions.append(predicted.cpu().numpy())\n",
    "            total += labels.size(0)\n",
    "            num_correct += (predicted == labels).sum()\n",
    "        print(f'Test Accuracy of the model: {float(num_correct)/float(total)*100:.2f}')    \n",
    "        true_labels = np.hstack(true_labels)\n",
    "        predictions = np.hstack(predictions)\n",
    "\n",
    "    return true_labels, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1ea30c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x, y - find the img from class x labelled as class y \n",
    "def test(model, dl, x, y, normalize=True):\n",
    "    model.eval()\n",
    "    true_labels = []\n",
    "    predictions = []\n",
    "    images_list = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in dl:\n",
    "            images_list.append(images.cpu().numpy())\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            predicted = torch.argmax(outputs.data,-1)        \n",
    "            true_labels.append(labels.cpu().numpy())\n",
    "            predictions.append(predicted.cpu().numpy())\n",
    "    \n",
    "    for n in range(60):\n",
    "        for i in range(32):\n",
    "            if (true_labels[n][i] == x)  & (predictions[n][i] == y):\n",
    "                #inv_tensor = inv_normalize(image_list[n][i]])\n",
    "                plt.imshow(np.transpose(images_list[n][i], (1, 2, 0)))\n",
    "                plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7390a77c",
   "metadata": {},
   "source": [
    "# Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "87d2dcae",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_list = [1, 2]\n",
    "learning_rate_list = [0.000559, 0.0001]\n",
    "batch_size_list = [64]\n",
    "optimizer_list = [\n",
    "    'Adam', \n",
    "    'AdamW', \n",
    "#     'SGD'\n",
    "]\n",
    "\n",
    "model_list = [\n",
    "    'resnet',\n",
    "#     'densenet'\n",
    "]\n",
    "\n",
    "grid_list = [\n",
    "    epochs_list,\n",
    "    learning_rate_list,\n",
    "    batch_size_list,\n",
    "    optimizer_list,\n",
    "    model_list\n",
    "            ]\n",
    "\n",
    "grid = list(itertools.product(*grid_list))\n",
    "random.shuffle(grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e90fdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  1. epochs=1 \n",
      "     learning_rate=0.000559 \n",
      "     batch_size=64 \n",
      "     optimizer=Adam\n",
      "     model=resnet\n",
      "        \n",
      "Params to learn:\n",
      "\t fc.weight\n",
      "\t fc.bias\n",
      "\n",
      "Epoch 0/0\n",
      "----------\n",
      "train Loss: 1.1787 Acc: 0.5297 f1_score: 0.6488\n"
     ]
    }
   ],
   "source": [
    "norm_mean = (0.676, 0.542, 0.519)\n",
    "norm_std = (0.290, 0.226, 0.237)\n",
    "num_workers = 16\n",
    "feature_extract=True\n",
    "num_loop = 2\n",
    "\n",
    "results_df = pd.DataFrame(columns=[\n",
    "    'dataset',\n",
    "    'num_classes',\n",
    "    'model',\n",
    "    'val_accuracy',\n",
    "    'train_time (min)',\n",
    "    'epochs',\n",
    "    'learning_rate',\n",
    "    'optimizer',\n",
    "    'batch_size',\n",
    "])\n",
    "\n",
    "for i in range(num_loop):\n",
    "    print(f'''\n",
    "  {i+1}. epochs={grid[i][0]} \n",
    "     learning_rate={grid[i][1]} \n",
    "     batch_size={grid[i][2]} \n",
    "     optimizer={grid[i][3]}\n",
    "     model={grid[i][4]}\n",
    "        ''') \n",
    "    start_time = time.time()#datetime.now(timezone('America/Chicago')).strftime('%Y%m%d-%H%M%S')\n",
    "    model_ft, input_size = initialize_model(model_name=grid[i][4], num_classes=len(classes), \n",
    "                                            feature_extract=feature_extract)\n",
    "    \n",
    "    train_dataset =ImageFolder(root = DIR_TRAIN, transform=transforms.Compose([\n",
    "        transforms.Resize((input_size, input_size)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomVerticalFlip(),\n",
    "        transforms.RandomRotation(20),\n",
    "        transforms.RandomCrop(size=(input_size, input_size)),\n",
    "        transforms.ToTensor(), transforms.Normalize(norm_mean, norm_std)]))\n",
    "\n",
    "    valid_dataset = ImageFolder(root = DIR_VAL, transform=transforms.Compose([\n",
    "        transforms.Resize((input_size, input_size)), \n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(norm_mean, norm_std)]))\n",
    "\n",
    "    test_dataset = ImageFolder(root = DIR_TEST, transform=transforms.Compose([\n",
    "        transforms.Resize((input_size,input_size)), \n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(norm_mean, norm_std)]))\n",
    "    \n",
    "    dataloaders_dict = {}\n",
    "    dataloaders_dict['train'] = DataLoader(train_dataset, batch_size=grid[i][2], \n",
    "                                           shuffle=True, num_workers=num_workers)\n",
    "    dataloaders_dict['val'] = DataLoader(valid_dataset, batch_size=grid[i][2], \n",
    "                                         shuffle=False, num_workers=num_workers, drop_last=False)\n",
    "    dataloader_test = DataLoader(test_dataset, batch_size=grid[i][2], shuffle=False, \n",
    "                                 num_workers=num_workers, drop_last=False)\n",
    "    \n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    model_ft = model_ft.to(device)\n",
    "    \n",
    "    # Gather the parameters to be optimized/updated in this run. If we are\n",
    "    #  fine tuning we will be updating all parameters. However, if we are \n",
    "    #  doing feature extract method, we will only update the parameters\n",
    "    #  that we have just initialized, i.e. the parameters with requires_grad\n",
    "    #  is True.\n",
    "    params_to_update = model_ft.parameters()\n",
    "    print(\"Params to learn:\")\n",
    "    if feature_extract:\n",
    "        params_to_update = []\n",
    "        for name,param in model_ft.named_parameters():\n",
    "            if param.requires_grad == True:\n",
    "                params_to_update.append(param)\n",
    "                print(\"\\t\",name)\n",
    "    else:\n",
    "        for name,param in model_ft.named_parameters():\n",
    "            if param.requires_grad == True:\n",
    "                print(\"\\t\",name)\n",
    "                             \n",
    "    optimizer_ft = getattr(torch.optim, grid[i][3])(model_ft.parameters(), lr=grid[i][1])\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    model_ft, hist = train_model(model_ft, dataloaders_dict, criterion, optimizer_ft, \n",
    "                                 num_epochs=grid[i][0], patience=3)\n",
    "    \n",
    "    end_time = time.time() - start_time\n",
    "    \n",
    "    row = {\n",
    "        'dataset': data_split,\n",
    "        'num_classes': int(len(classes)),\n",
    "        'model': grid[i][4],\n",
    "        'val_accuracy': f'{hist[0].item():.2f}',\n",
    "        'train_time (min)': f'{end_time/60:.0f}',\n",
    "        'epochs': int(grid[i][0]),\n",
    "        'learning_rate': grid[i][1],\n",
    "        'optimizer': grid[i][3],\n",
    "        'batch_size': int(grid[i][2])\n",
    "    }\n",
    "    \n",
    "    new_df = pd.DataFrame([row])\n",
    "    results_df = pd.concat([results_df, new_df], axis=0, ignore_index=True).sort_values('val_accuracy', ascending = False)\n",
    "\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab859c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = f'{data_split}_resnet50'\n",
    "torch.save(model, f'model/{model_name}.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5b2009",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load(f'model/{model_name}.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5195b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_labels, predictions = test_model(model, dataloader_test, normalize=True)\n",
    "c_matrix = confusion_matrix(true_labels, predictions, normalize='true')\n",
    "plt.figure(figsize=(12, 12))\n",
    "plt.title(\"Confusion matrix\")\n",
    "sns.heatmap(c_matrix, cmap='Blues', annot=True, xticklabels=classes, yticklabels=classes, fmt='.1%', cbar=True)\n",
    "plt.xlabel('predictions')\n",
    "plt.ylabel('true labels')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af69f8c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # False prediction\n",
    "# test(model, dataloader_test, 2, 0) #(potentially malignant skin tumors, non-cancerous skin condition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b6a2a4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Correct prediction of autoimmune disorder\n",
    "# test(model, dataloader_test, 3, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "493a1eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Trace model\n",
    "\n",
    "# # must be the same size a minibatch with 1 example image\n",
    "# example = torch.rand(1, 3, 224, 224)\n",
    "\n",
    "# # move model back to cpu, do tracing, and optimize\n",
    "# model_conv = model.to('cpu')\n",
    "# traced_script_module = torch.jit.trace(model_conv, example)\n",
    "# torchscript_model_optimized = optimize_for_mobile(traced_script_module)\n",
    "\n",
    "# # save optimized model for mobile\n",
    "# PATH = f'model/{model_name}_traced.pt'\n",
    "# torchscript_model_optimized.save(PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ef8060",
   "metadata": {},
   "source": [
    "# Conversion from PyTorch to CoreML\n",
    "https://github.com/vincentfpgarcia/from-pytorch-to-coreml/blob/master/step6_part1.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc78908e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import coremltools as ct\n",
    "from coremltools.converters import ClassifierConfig\n",
    "\n",
    "classifier_config = ClassifierConfig(class_labels=class_list)\n",
    "\n",
    "model = torch.load(f'model/{model_name}.pt')\n",
    "\n",
    "# Create dummy input\n",
    "dummy_input = torch.rand(1, 3, 224, 224)\n",
    "\n",
    "model_conv = model.to('cpu')\n",
    "\n",
    "# Trace the model\n",
    "traced_model = torch.jit.trace(model_conv, dummy_input)\n",
    "\n",
    "# Create the input image type\n",
    "input_image = ct.ImageType(name=\"my_input\", shape=(1, 3, 224, 224), scale=1/255)\n",
    "\n",
    "# Convert the model\n",
    "coreml_model = ct.convert(traced_model, inputs=[input_image], classifier_config=classifier_config)\n",
    "\n",
    "# Modify the output's name to \"my_output\" in the spec\n",
    "spec = coreml_model.get_spec()\n",
    "ct.utils.rename_feature(spec, \"var_840\", \"my_output\")\n",
    "\n",
    "# Re-create the model from the updated spec\n",
    "coreml_model_updated = ct.models.MLModel(spec)\n",
    "\n",
    "# Save the CoreML model\n",
    "coremlmodel_name = 'skindiseases5'\n",
    "coreml_model_updated.save(f'model/{coremlmodel_name}.mlmodel')\n",
    "\n",
    "# Load the CoreML model\n",
    "model =  ct.models.MLModel(f'model/{coremlmodel_name}.mlmodel')\n",
    "\n",
    "# Display its specifications\n",
    "print()\n",
    "print(model)\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "# classes\n",
    "classes = ['Benign Marking or Mole',\n",
    "           'Non-Cancerous Skin Condition',\n",
    "           'Potentially Malignant Skin Tumors', \n",
    "           'Toxin, Fungal, Bug, Viral, or Bacterial Infections',\n",
    "           'Unclassified']\n",
    "\n",
    "# Load the test image\n",
    "image = Image.open('inference/Potentially Malignant Skin Tumors/melanoma.jpeg')\n",
    "\n",
    "# Prediction vector as a numpy array\n",
    "pred = model.predict({'my_input': image.resize((224, 224))})\n",
    "pred = pred['my_output']\n",
    "pred = pred.squeeze()\n",
    "\n",
    "# Display the most probable class\n",
    "idx = pred.argmax()\n",
    "print('Predicted class : %d (%s)' % (idx, classes[idx]))\n",
    "pred['classLabel']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e4e4e7",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feebb405",
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_dataset = ImageFolder(root = 'inference/', transform=transforms.Compose([\n",
    "    transforms.Resize(255),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.676, 0.542, 0.519], std=[0.290, 0.226, 0.237])\n",
    "]))\n",
    "\n",
    "dataloader_inference = DataLoader(inference_dataset, batch_size=1, \n",
    "                                  shuffle=False, num_workers=1, drop_last=False)\n",
    "\n",
    "def inference_model(model, dl, normalize=True):\n",
    "    model.eval()\n",
    "    true_labels = []\n",
    "    predictions = []\n",
    "    total = 0\n",
    "    num_correct = 0\n",
    "    # device = torch.device('cpu')\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in dl:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            predicted = torch.argmax(outputs.data,-1)        \n",
    "            true_labels.append(labels.cpu().numpy())\n",
    "            predictions.append(predicted.cpu().numpy())\n",
    "            total += labels.size(0)\n",
    "            num_correct += (predicted == labels).sum()\n",
    "        print(f'Inference Accuracy of the model: {float(num_correct)/float(total)*100:.2f}')    \n",
    "        true_labels = np.hstack(true_labels)\n",
    "        predictions = np.hstack(predictions)\n",
    "\n",
    "    return true_labels, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b23eee06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = torch.jit.load(f'model/{model_name}_traced.pt')\n",
    "model = torch.load(f'model/{model_name}.pt')\n",
    "true_labels, predictions = inference_model(model, dataloader_inference, normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa38c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de2b5dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "269f51c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !tar -czf model/model.tar.gz model/merged_resnet50.pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30fb9c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !aws s3 cp model/model.tar.gz s3://rubyhan-w210-datasets/model/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "w210_env",
   "language": "python",
   "name": "w210_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
