{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b1aac74",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8fa1a9f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/SageMaker/teledermatologyAI_capstone\n"
     ]
    }
   ],
   "source": [
    "cd /home/ec2-user/SageMaker/teledermatologyAI_capstone/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "97d06834",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installed kernelspec w210env in /home/ec2-user/.local/share/jupyter/kernels/w210env\n"
     ]
    }
   ],
   "source": [
    "# !python3 -m venv w210env\n",
    "!source w210env/bin/activate & ipython kernel install --user --name=w210env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cde9b0b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Requirement already satisfied: pandas in /home/ec2-user/anaconda3/envs/JupyterSystemEnv/lib/python3.7/site-packages (from -r requirements.txt (line 1)) (1.3.5)\n",
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.5.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (11.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.2/11.2 MB\u001b[0m \u001b[31m21.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting scikit-image\n",
      "  Downloading scikit_image-0.19.3-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (13.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.5/13.5 MB\u001b[0m \u001b[31m45.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting basic-image-eda\n",
      "  Using cached basic_image_eda-0.0.3-py3-none-any.whl (9.8 kB)\n",
      "Collecting seaborn\n",
      "  Downloading seaborn-0.12.1-py3-none-any.whl (288 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m288.2/288.2 kB\u001b[0m \u001b[31m58.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting torchvision\n",
      "  Downloading torchvision-0.14.0-cp37-cp37m-manylinux1_x86_64.whl (24.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.3/24.3 MB\u001b[0m \u001b[31m49.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting sklearn\n",
      "  Downloading sklearn-0.0.tar.gz (1.1 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting pandas_profiling\n",
      "  Using cached pandas_profiling-3.4.0-py2.py3-none-any.whl (315 kB)\n",
      "Collecting awswrangler\n",
      "  Using cached awswrangler-2.17.0-py3-none-any.whl (251 kB)\n",
      "Requirement already satisfied: ipywidgets in /home/ec2-user/anaconda3/envs/JupyterSystemEnv/lib/python3.7/site-packages (from -r requirements.txt (line 10)) (8.0.2)\n",
      "Collecting coremltools\n",
      "  Downloading coremltools-6.0-cp37-none-manylinux1_x86_64.whl (1.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hCollecting torch_lr_finder\n",
      "  Using cached torch_lr_finder-0.2.1-py3-none-any.whl (11 kB)\n",
      "Collecting pytorchtools\n",
      "  Using cached pytorchtools-0.0.2-py2.py3-none-any.whl (3.1 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /home/ec2-user/anaconda3/envs/JupyterSystemEnv/lib/python3.7/site-packages (from pandas->-r requirements.txt (line 1)) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /home/ec2-user/anaconda3/envs/JupyterSystemEnv/lib/python3.7/site-packages (from pandas->-r requirements.txt (line 1)) (2022.5)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /home/ec2-user/anaconda3/envs/JupyterSystemEnv/lib/python3.7/site-packages (from pandas->-r requirements.txt (line 1)) (1.21.6)\n",
      "Collecting cycler>=0.10\n",
      "  Downloading cycler-0.11.0-py3-none-any.whl (6.4 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ec2-user/anaconda3/envs/JupyterSystemEnv/lib/python3.7/site-packages (from matplotlib->-r requirements.txt (line 2)) (21.3)\n",
      "Collecting pillow>=6.2.0\n",
      "  Downloading Pillow-9.3.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m44.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hCollecting kiwisolver>=1.0.1\n",
      "  Downloading kiwisolver-1.4.4-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pyparsing>=2.2.1 in /home/ec2-user/anaconda3/envs/JupyterSystemEnv/lib/python3.7/site-packages (from matplotlib->-r requirements.txt (line 2)) (3.0.9)\n",
      "Collecting fonttools>=4.22.0\n",
      "  Downloading fonttools-4.38.0-py3-none-any.whl (965 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m965.4/965.4 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting scipy>=1.4.1\n",
      "  Downloading scipy-1.7.3-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (38.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.1/38.1 MB\u001b[0m \u001b[31m38.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting imageio>=2.4.1\n",
      "  Downloading imageio-2.22.3-py3-none-any.whl (3.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m36.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting networkx>=2.2\n",
      "  Downloading networkx-2.6.3-py3-none-any.whl (1.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m50.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hCollecting tifffile>=2019.7.26\n",
      "  Downloading tifffile-2021.11.2-py3-none-any.whl (178 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m178.9/178.9 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting PyWavelets>=1.1.1\n",
      "  Downloading PyWavelets-1.3.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.4/6.4 MB\u001b[0m \u001b[31m75.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing_extensions in /home/ec2-user/anaconda3/envs/JupyterSystemEnv/lib/python3.7/site-packages (from seaborn->-r requirements.txt (line 5)) (4.4.0)\n",
      "Requirement already satisfied: requests in /home/ec2-user/anaconda3/envs/JupyterSystemEnv/lib/python3.7/site-packages (from torchvision->-r requirements.txt (line 6)) (2.28.1)\n",
      "Collecting torch==1.13.0\n",
      "  Downloading torch-1.13.0-cp37-cp37m-manylinux1_x86_64.whl (890.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m890.2/890.2 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-nvrtc-cu11==11.7.99\n",
      "  Using cached nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n",
      "Collecting nvidia-cuda-runtime-cu11==11.7.99\n",
      "  Using cached nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n",
      "Collecting nvidia-cublas-cu11==11.10.3.66\n",
      "  Using cached nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
      "Collecting nvidia-cudnn-cu11==8.5.0.96\n",
      "  Using cached nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n",
      "Requirement already satisfied: wheel in /home/ec2-user/anaconda3/envs/JupyterSystemEnv/lib/python3.7/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch==1.13.0->torchvision->-r requirements.txt (line 6)) (0.37.1)\n",
      "Requirement already satisfied: setuptools in /home/ec2-user/anaconda3/envs/JupyterSystemEnv/lib/python3.7/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch==1.13.0->torchvision->-r requirements.txt (line 6)) (65.5.0)\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.0.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (24.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.8/24.8 MB\u001b[0m \u001b[31m48.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting tqdm<4.65,>=4.48.2\n",
      "  Downloading tqdm-4.64.1-py2.py3-none-any.whl (78 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.5/78.5 kB\u001b[0m \u001b[31m24.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting htmlmin==0.1.12\n",
      "  Using cached htmlmin-0.1.12.tar.gz (19 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting statsmodels<0.14,>=0.13.2\n",
      "  Downloading statsmodels-0.13.5-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m25.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting missingno<0.6,>=0.4.2\n",
      "  Using cached missingno-0.5.1-py3-none-any.whl (8.7 kB)\n",
      "Collecting pydantic<1.11,>=1.8.1\n",
      "  Downloading pydantic-1.10.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.8/11.8 MB\u001b[0m \u001b[31m69.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting visions[type_image_path]==0.7.5\n",
      "  Using cached visions-0.7.5-py3-none-any.whl (102 kB)\n",
      "Collecting phik<0.13,>=0.11.1\n",
      "  Downloading phik-0.12.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (690 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m690.3/690.3 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: jinja2<3.2,>=2.11.1 in /home/ec2-user/anaconda3/envs/JupyterSystemEnv/lib/python3.7/site-packages (from pandas_profiling->-r requirements.txt (line 8)) (3.0.0)\n",
      "Collecting multimethod<1.10,>=1.4\n",
      "  Using cached multimethod-1.9-py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: PyYAML<6.1,>=5.0.0 in /home/ec2-user/anaconda3/envs/JupyterSystemEnv/lib/python3.7/site-packages (from pandas_profiling->-r requirements.txt (line 8)) (5.4.1)\n",
      "Requirement already satisfied: attrs>=19.3.0 in /home/ec2-user/anaconda3/envs/JupyterSystemEnv/lib/python3.7/site-packages (from visions[type_image_path]==0.7.5->pandas_profiling->-r requirements.txt (line 8)) (22.1.0)\n",
      "Collecting tangled-up-in-unicode>=0.0.4\n",
      "  Using cached tangled_up_in_unicode-0.2.0-py3-none-any.whl (4.7 MB)\n",
      "Collecting imagehash\n",
      "  Using cached ImageHash-4.3.1-py2.py3-none-any.whl (296 kB)\n",
      "Collecting pymysql<2.0.0,>=1.0.0\n",
      "  Using cached PyMySQL-1.0.2-py3-none-any.whl (43 kB)\n",
      "Collecting jsonpath-ng<2.0.0,>=1.5.3\n",
      "  Using cached jsonpath_ng-1.5.3-py3-none-any.whl (29 kB)\n",
      "Collecting redshift-connector<2.1.0,>=2.0.889\n",
      "  Using cached redshift_connector-2.0.909-py3-none-any.whl (112 kB)\n",
      "Collecting pg8000<2.0.0,>=1.20.0\n",
      "  Using cached pg8000-1.29.3-py3-none-any.whl (51 kB)\n",
      "Collecting requests-aws4auth<2.0.0,>=1.1.1\n",
      "  Using cached requests_aws4auth-1.1.2-py2.py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: botocore<2.0.0,>=1.27.11 in /home/ec2-user/anaconda3/envs/JupyterSystemEnv/lib/python3.7/site-packages (from awswrangler->-r requirements.txt (line 9)) (1.27.96)\n",
      "Collecting gremlinpython<4.0.0,>=3.5.2\n",
      "  Using cached gremlinpython-3.6.1-py2.py3-none-any.whl (73 kB)\n",
      "Collecting openpyxl<3.1.0,>=3.0.0\n",
      "  Downloading openpyxl-3.0.10-py2.py3-none-any.whl (242 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m242.1/242.1 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pyarrow<8.1.0,>=2.0.0\n",
      "  Downloading pyarrow-8.0.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (29.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m29.3/29.3 MB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting opensearch-py<3,>=1\n",
      "  Using cached opensearch_py-2.0.0-py2.py3-none-any.whl (204 kB)\n",
      "Collecting progressbar2<5.0.0,>=4.0.0\n",
      "  Using cached progressbar2-4.2.0-py2.py3-none-any.whl (27 kB)\n",
      "Requirement already satisfied: boto3<2.0.0,>=1.24.11 in /home/ec2-user/anaconda3/envs/JupyterSystemEnv/lib/python3.7/site-packages (from awswrangler->-r requirements.txt (line 9)) (1.24.96)\n",
      "Collecting backoff<3.0.0,>=1.11.1\n",
      "  Using cached backoff-2.2.1-py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: ipykernel>=4.5.1 in /home/ec2-user/anaconda3/envs/JupyterSystemEnv/lib/python3.7/site-packages (from ipywidgets->-r requirements.txt (line 10)) (5.5.6)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /home/ec2-user/anaconda3/envs/JupyterSystemEnv/lib/python3.7/site-packages (from ipywidgets->-r requirements.txt (line 10)) (7.33.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /home/ec2-user/anaconda3/envs/JupyterSystemEnv/lib/python3.7/site-packages (from ipywidgets->-r requirements.txt (line 10)) (5.5.0)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0 in /home/ec2-user/anaconda3/envs/JupyterSystemEnv/lib/python3.7/site-packages (from ipywidgets->-r requirements.txt (line 10)) (4.0.3)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0 in /home/ec2-user/anaconda3/envs/JupyterSystemEnv/lib/python3.7/site-packages (from ipywidgets->-r requirements.txt (line 10)) (3.0.3)\n",
      "Collecting protobuf<=3.20.1,>=3.1.0\n",
      "  Downloading protobuf-3.20.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hCollecting sympy\n",
      "  Downloading sympy-1.10.1-py3-none-any.whl (6.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.4/6.4 MB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: jmespath<2.0.0,>=0.7.1 in /home/ec2-user/anaconda3/envs/JupyterSystemEnv/lib/python3.7/site-packages (from boto3<2.0.0,>=1.24.11->awswrangler->-r requirements.txt (line 9)) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /home/ec2-user/anaconda3/envs/JupyterSystemEnv/lib/python3.7/site-packages (from boto3<2.0.0,>=1.24.11->awswrangler->-r requirements.txt (line 9)) (0.6.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /home/ec2-user/anaconda3/envs/JupyterSystemEnv/lib/python3.7/site-packages (from botocore<2.0.0,>=1.27.11->awswrangler->-r requirements.txt (line 9)) (1.26.8)\n",
      "Collecting aiohttp<=3.8.1,>=3.8.0\n",
      "  Downloading aiohttp-3.8.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hCollecting aenum<4.0.0,>=1.4.5\n",
      "  Using cached aenum-3.1.11-py3-none-any.whl (131 kB)\n",
      "Collecting isodate<1.0.0,>=0.6.0\n",
      "  Using cached isodate-0.6.1-py2.py3-none-any.whl (41 kB)\n",
      "Requirement already satisfied: nest-asyncio in /home/ec2-user/anaconda3/envs/JupyterSystemEnv/lib/python3.7/site-packages (from gremlinpython<4.0.0,>=3.5.2->awswrangler->-r requirements.txt (line 9)) (1.5.6)\n",
      "Requirement already satisfied: ipython-genutils in /home/ec2-user/anaconda3/envs/JupyterSystemEnv/lib/python3.7/site-packages (from ipykernel>=4.5.1->ipywidgets->-r requirements.txt (line 10)) (0.2.0)\n",
      "Requirement already satisfied: tornado>=4.2 in /home/ec2-user/anaconda3/envs/JupyterSystemEnv/lib/python3.7/site-packages (from ipykernel>=4.5.1->ipywidgets->-r requirements.txt (line 10)) (6.2)\n",
      "Requirement already satisfied: jupyter-client in /home/ec2-user/anaconda3/envs/JupyterSystemEnv/lib/python3.7/site-packages (from ipykernel>=4.5.1->ipywidgets->-r requirements.txt (line 10)) (6.1.12)\n",
      "Requirement already satisfied: decorator in /home/ec2-user/anaconda3/envs/JupyterSystemEnv/lib/python3.7/site-packages (from ipython>=6.1.0->ipywidgets->-r requirements.txt (line 10)) (5.1.1)\n",
      "Requirement already satisfied: pygments in /home/ec2-user/anaconda3/envs/JupyterSystemEnv/lib/python3.7/site-packages (from ipython>=6.1.0->ipywidgets->-r requirements.txt (line 10)) (2.13.0)\n",
      "Requirement already satisfied: pexpect>4.3 in /home/ec2-user/anaconda3/envs/JupyterSystemEnv/lib/python3.7/site-packages (from ipython>=6.1.0->ipywidgets->-r requirements.txt (line 10)) (4.8.0)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /home/ec2-user/anaconda3/envs/JupyterSystemEnv/lib/python3.7/site-packages (from ipython>=6.1.0->ipywidgets->-r requirements.txt (line 10)) (3.0.31)\n",
      "Requirement already satisfied: backcall in /home/ec2-user/anaconda3/envs/JupyterSystemEnv/lib/python3.7/site-packages (from ipython>=6.1.0->ipywidgets->-r requirements.txt (line 10)) (0.2.0)\n",
      "Requirement already satisfied: matplotlib-inline in /home/ec2-user/anaconda3/envs/JupyterSystemEnv/lib/python3.7/site-packages (from ipython>=6.1.0->ipywidgets->-r requirements.txt (line 10)) (0.1.6)\n",
      "Requirement already satisfied: jedi>=0.16 in /home/ec2-user/anaconda3/envs/JupyterSystemEnv/lib/python3.7/site-packages (from ipython>=6.1.0->ipywidgets->-r requirements.txt (line 10)) (0.18.1)\n",
      "Requirement already satisfied: pickleshare in /home/ec2-user/anaconda3/envs/JupyterSystemEnv/lib/python3.7/site-packages (from ipython>=6.1.0->ipywidgets->-r requirements.txt (line 10)) (0.7.5)\n",
      "Requirement already satisfied: MarkupSafe>=2.0.0rc2 in /home/ec2-user/anaconda3/envs/JupyterSystemEnv/lib/python3.7/site-packages (from jinja2<3.2,>=2.11.1->pandas_profiling->-r requirements.txt (line 8)) (2.1.1)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/JupyterSystemEnv/lib/python3.7/site-packages (from jsonpath-ng<2.0.0,>=1.5.3->awswrangler->-r requirements.txt (line 9)) (1.16.0)\n",
      "Collecting ply\n",
      "  Downloading ply-3.11-py2.py3-none-any.whl (49 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.6/49.6 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting et-xmlfile\n",
      "  Downloading et_xmlfile-1.1.0-py3-none-any.whl (4.7 kB)\n",
      "Requirement already satisfied: certifi in /home/ec2-user/anaconda3/envs/JupyterSystemEnv/lib/python3.7/site-packages (from opensearch-py<3,>=1->awswrangler->-r requirements.txt (line 9)) (2022.9.24)\n",
      "Requirement already satisfied: importlib-metadata>=1.0 in /home/ec2-user/anaconda3/envs/JupyterSystemEnv/lib/python3.7/site-packages (from pg8000<2.0.0,>=1.20.0->awswrangler->-r requirements.txt (line 9)) (4.11.4)\n",
      "Collecting scramp>=1.4.3\n",
      "  Using cached scramp-1.4.4-py3-none-any.whl (13 kB)\n",
      "Collecting joblib>=0.14.1\n",
      "  Downloading joblib-1.2.0-py3-none-any.whl (297 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m298.0/298.0 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting python-utils>=3.0.0\n",
      "  Using cached python_utils-3.4.5-py2.py3-none-any.whl (23 kB)\n",
      "Collecting lxml>=4.6.5\n",
      "  Downloading lxml-4.9.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (6.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.4/6.4 MB\u001b[0m \u001b[31m25.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting beautifulsoup4<5.0.0,>=4.7.0\n",
      "  Downloading beautifulsoup4-4.11.1-py3-none-any.whl (128 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.2/128.2 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/JupyterSystemEnv/lib/python3.7/site-packages (from requests->torchvision->-r requirements.txt (line 6)) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /home/ec2-user/anaconda3/envs/JupyterSystemEnv/lib/python3.7/site-packages (from requests->torchvision->-r requirements.txt (line 6)) (2.1.1)\n",
      "Collecting patsy>=0.5.2\n",
      "  Downloading patsy-0.5.3-py2.py3-none-any.whl (233 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.8/233.8 kB\u001b[0m \u001b[31m52.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting threadpoolctl>=2.0.0\n",
      "  Downloading threadpoolctl-3.1.0-py3-none-any.whl (14 kB)\n",
      "Collecting mpmath>=0.19\n",
      "  Downloading mpmath-1.2.1-py3-none-any.whl (532 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m532.6/532.6 kB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /home/ec2-user/anaconda3/envs/JupyterSystemEnv/lib/python3.7/site-packages (from aiohttp<=3.8.1,>=3.8.0->gremlinpython<4.0.0,>=3.5.2->awswrangler->-r requirements.txt (line 9)) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/ec2-user/anaconda3/envs/JupyterSystemEnv/lib/python3.7/site-packages (from aiohttp<=3.8.1,>=3.8.0->gremlinpython<4.0.0,>=3.5.2->awswrangler->-r requirements.txt (line 9)) (1.7.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/ec2-user/anaconda3/envs/JupyterSystemEnv/lib/python3.7/site-packages (from aiohttp<=3.8.1,>=3.8.0->gremlinpython<4.0.0,>=3.5.2->awswrangler->-r requirements.txt (line 9)) (1.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/ec2-user/anaconda3/envs/JupyterSystemEnv/lib/python3.7/site-packages (from aiohttp<=3.8.1,>=3.8.0->gremlinpython<4.0.0,>=3.5.2->awswrangler->-r requirements.txt (line 9)) (6.0.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/ec2-user/anaconda3/envs/JupyterSystemEnv/lib/python3.7/site-packages (from aiohttp<=3.8.1,>=3.8.0->gremlinpython<4.0.0,>=3.5.2->awswrangler->-r requirements.txt (line 9)) (1.3.1)\n",
      "Requirement already satisfied: asynctest==0.13.0 in /home/ec2-user/anaconda3/envs/JupyterSystemEnv/lib/python3.7/site-packages (from aiohttp<=3.8.1,>=3.8.0->gremlinpython<4.0.0,>=3.5.2->awswrangler->-r requirements.txt (line 9)) (0.13.0)\n",
      "Collecting soupsieve>1.2\n",
      "  Downloading soupsieve-2.3.2.post1-py3-none-any.whl (37 kB)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/envs/JupyterSystemEnv/lib/python3.7/site-packages (from importlib-metadata>=1.0->pg8000<2.0.0,>=1.20.0->awswrangler->-r requirements.txt (line 9)) (3.9.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /home/ec2-user/anaconda3/envs/JupyterSystemEnv/lib/python3.7/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets->-r requirements.txt (line 10)) (0.8.3)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /home/ec2-user/anaconda3/envs/JupyterSystemEnv/lib/python3.7/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets->-r requirements.txt (line 10)) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /home/ec2-user/anaconda3/envs/JupyterSystemEnv/lib/python3.7/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=6.1.0->ipywidgets->-r requirements.txt (line 10)) (0.2.5)\n",
      "Collecting asn1crypto>=1.5.1\n",
      "  Using cached asn1crypto-1.5.1-py2.py3-none-any.whl (105 kB)\n",
      "Requirement already satisfied: jupyter-core>=4.6.0 in /home/ec2-user/anaconda3/envs/JupyterSystemEnv/lib/python3.7/site-packages (from jupyter-client->ipykernel>=4.5.1->ipywidgets->-r requirements.txt (line 10)) (4.11.1)\n",
      "Requirement already satisfied: pyzmq>=13 in /home/ec2-user/anaconda3/envs/JupyterSystemEnv/lib/python3.7/site-packages (from jupyter-client->ipykernel>=4.5.1->ipywidgets->-r requirements.txt (line 10)) (24.0.1)\n",
      "Building wheels for collected packages: sklearn, htmlmin\n",
      "  Building wheel for sklearn (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sklearn: filename=sklearn-0.0-py2.py3-none-any.whl size=1304 sha256=e4a6de40a8917972a69391231da7d95b902fea22933d6a76bfd2dcbec4f9b235\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/f5/d9/b2/a9d58f54cfa2235cb19895aeb4e5d8488667c0536c7248f212\n",
      "  Building wheel for htmlmin (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for htmlmin: filename=htmlmin-0.1.12-py3-none-any.whl size=27082 sha256=4ea1bcb3ff4c58f5a9ce2177bd8cfaf28ddea650d01a5182419994495037b6d5\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/56/18/c1/6b3058b1db1f804221515aac5ecf2513d5ba971f33b8560c7a\n",
      "Successfully built sklearn htmlmin\n",
      "Installing collected packages: pytorchtools, ply, mpmath, htmlmin, basic-image-eda, asn1crypto, aenum, tqdm, tifffile, threadpoolctl, tangled-up-in-unicode, sympy, soupsieve, scipy, PyWavelets, python-utils, pymysql, pydantic, pyarrow, protobuf, pillow, patsy, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cublas-cu11, networkx, multimethod, lxml, kiwisolver, jsonpath-ng, joblib, isodate, fonttools, et-xmlfile, cycler, backoff, scramp, scikit-learn, requests-aws4auth, progressbar2, opensearch-py, openpyxl, nvidia-cudnn-cu11, matplotlib, imageio, imagehash, coremltools, beautifulsoup4, aiohttp, visions, torch, statsmodels, sklearn, seaborn, scikit-image, phik, pg8000, gremlinpython, torchvision, torch_lr_finder, missingno, redshift-connector, pandas_profiling, awswrangler\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 3.20.3\n",
      "    Uninstalling protobuf-3.20.3:\n",
      "      Successfully uninstalled protobuf-3.20.3\n",
      "  Attempting uninstall: aiohttp\n",
      "    Found existing installation: aiohttp 3.8.3\n",
      "    Uninstalling aiohttp-3.8.3:\n",
      "      Successfully uninstalled aiohttp-3.8.3\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "aiobotocore 2.4.0 requires botocore<1.27.60,>=1.27.59, but you have botocore 1.27.96 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed PyWavelets-1.3.0 aenum-3.1.11 aiohttp-3.8.1 asn1crypto-1.5.1 awswrangler-2.17.0 backoff-2.2.1 basic-image-eda-0.0.3 beautifulsoup4-4.11.1 coremltools-6.0 cycler-0.11.0 et-xmlfile-1.1.0 fonttools-4.38.0 gremlinpython-3.6.1 htmlmin-0.1.12 imagehash-4.3.1 imageio-2.22.3 isodate-0.6.1 joblib-1.2.0 jsonpath-ng-1.5.3 kiwisolver-1.4.4 lxml-4.9.1 matplotlib-3.5.3 missingno-0.5.1 mpmath-1.2.1 multimethod-1.9 networkx-2.6.3 nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 openpyxl-3.0.10 opensearch-py-2.0.0 pandas_profiling-3.4.0 patsy-0.5.3 pg8000-1.29.3 phik-0.12.2 pillow-9.3.0 ply-3.11 progressbar2-4.2.0 protobuf-3.20.1 pyarrow-8.0.0 pydantic-1.10.2 pymysql-1.0.2 python-utils-3.4.5 pytorchtools-0.0.2 redshift-connector-2.0.909 requests-aws4auth-1.1.2 scikit-image-0.19.3 scikit-learn-1.0.2 scipy-1.7.3 scramp-1.4.4 seaborn-0.12.1 sklearn-0.0 soupsieve-2.3.2.post1 statsmodels-0.13.5 sympy-1.10.1 tangled-up-in-unicode-0.2.0 threadpoolctl-3.1.0 tifffile-2021.11.2 torch-1.13.0 torch_lr_finder-0.2.1 torchvision-0.14.0 tqdm-4.64.1 visions-0.7.5\n"
     ]
    }
   ],
   "source": [
    "# !pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d87d3783",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_9942/343673033.py\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpytz\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtimezone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpytorchtools\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mEarlyStopping\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/SageMaker/teledermatologyAI_capstone/notebooks/modelling/pytorchtools.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mEarlyStopping\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;34m\"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import shutil\n",
    "import time\n",
    "import copy\n",
    "import random\n",
    "import itertools\n",
    "from datetime import datetime\n",
    "from pytz import timezone\n",
    "\n",
    "from pytorchtools import EarlyStopping\n",
    "\n",
    "import pandas as pd\n",
    "from pandas_profiling import ProfileReport\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mp_image\n",
    "import seaborn as sns\n",
    "\n",
    "from IPython.display import Image, display\n",
    "\n",
    "from torchvision.datasets import ImageFolder\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.mobile_optimizer import optimize_for_mobile\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch_lr_finder import LRFinder\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "\n",
    "import awswrangler as wr\n",
    "\n",
    "# expand pandas df rows/column widths etc.\n",
    "pd.set_option(\"display.max_rows\", None, # display all rows\n",
    "              \"display.max_columns\", None, # display all columns\n",
    "              \"display.max_colwidth\", None, # expand column width\n",
    "              \"display.html.use_mathjax\", False\n",
    "             ) # disable Latex style mathjax rendering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6818b1ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "cd /home/ec2-user/SageMaker/teledermatologyAI_capstone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c0614e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# credits: https://github.com/yuliyabohdan/Skin-diseases-classification-Dermnet-/blob/main/skin_diseases_clas_ResNet50.ipynb\n",
    "data_split = 'split_3'\n",
    "data_dir = 'data_class_folder3'\n",
    "\n",
    "DIR = data_dir\n",
    "DIR_TRAIN = f'{DIR}/train/'\n",
    "DIR_VAL = f'{DIR}/val/'\n",
    "DIR_TEST = f'{DIR}/test/' \n",
    "\n",
    "classes = sorted(os.listdir(DIR_TRAIN))\n",
    "print(f'Data split: {DIR}')\n",
    "print(f'Total classes: {len(classes)}')\n",
    "\n",
    "# total train, val and test images\n",
    "train_count = 0\n",
    "val_count = 0\n",
    "test_count = 0\n",
    "\n",
    "classes_df = []\n",
    "for _class in classes:\n",
    "    class_dict = {}\n",
    "    train_count += len(os.listdir(DIR_TRAIN + _class))\n",
    "    val_count += len(os.listdir(DIR_VAL + _class))\n",
    "    test_count += len(os.listdir(DIR_TEST + _class))\n",
    "    class_dict.update({'Class': _class, \n",
    "                       'Train': len(os.listdir(DIR_TRAIN + _class)),\n",
    "                       'Val': len(os.listdir(DIR_VAL + _class)),\n",
    "                       'Test': len(os.listdir(DIR_TEST + _class)) })\n",
    "    classes_df.append(class_dict)\n",
    "\n",
    "print(f'Total num train images: {train_count}')\n",
    "print(f'Total num val images: {val_count}')\n",
    "print(f'Total num test images: {test_count}')\n",
    "print(pd.DataFrame(classes_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "255a9624",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a33221",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_parameter_requires_grad(model, feature_extracting):\n",
    "    if feature_extracting:\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False\n",
    "            \n",
    "def initialize_model(model_name, num_classes, feature_extract=True):\n",
    "    # Initialize these variables which will be set in this if statement. Each of these\n",
    "    #   variables is model specific.\n",
    "    model_ft = None\n",
    "    input_size = 0\n",
    "\n",
    "    if model_name == \"resnet\":\n",
    "        \"\"\" Resnet50\n",
    "        \"\"\"\n",
    "        model_ft = models.resnet50(weights='DEFAULT')\n",
    "        #we can select any possible variation of ResNet such as Resnet18, Resnet34, Resnet50, Resnet101, and Resnet152\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.fc.in_features\n",
    "        model_ft.fc = nn.Linear(num_ftrs, num_classes)\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"alexnet\":\n",
    "        \"\"\" Alexnet\n",
    "        \"\"\"\n",
    "        model_ft = models.alexnet(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.classifier[6].in_features\n",
    "        model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"vgg\":\n",
    "        \"\"\" VGG11_bn\n",
    "        \"\"\"\n",
    "        model_ft = models.vgg11_bn(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.classifier[6].in_features\n",
    "        model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"squeezenet\":\n",
    "        \"\"\" Squeezenet\n",
    "        \"\"\"\n",
    "        model_ft = models.squeezenet1_0(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        model_ft.classifier[1] = nn.Conv2d(512, num_classes, kernel_size=(1,1), stride=(1,1))\n",
    "        model_ft.num_classes = num_classes\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"densenet\":\n",
    "        \"\"\" Densenet\n",
    "        \"\"\"\n",
    "        model_ft = models.densenet121(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.classifier.in_features\n",
    "        model_ft.classifier = nn.Linear(num_ftrs, num_classes) \n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"inception\":\n",
    "        \"\"\" Inception v3 \n",
    "        Be careful, expects (299,299) sized images and has auxiliary output\n",
    "        \"\"\"\n",
    "        model_ft = models.inception_v3(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        # Handle the auxilary net\n",
    "        num_ftrs = model_ft.AuxLogits.fc.in_features\n",
    "        model_ft.AuxLogits.fc = nn.Linear(num_ftrs, num_classes)\n",
    "        # Handle the primary net\n",
    "        num_ftrs = model_ft.fc.in_features\n",
    "        model_ft.fc = nn.Linear(num_ftrs,num_classes)\n",
    "        input_size = 299\n",
    "\n",
    "    else:\n",
    "        print(\"Invalid model name, exiting...\")\n",
    "        exit()\n",
    "\n",
    "    return model_ft, input_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b522455",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/67959327/how-to-calculate-the-f1-score\n",
    "def train_model(model, dataloaders, criterion, optimizer, num_epochs, patience):\n",
    "    since = time.time()\n",
    "\n",
    "    val_acc_history = []\n",
    "    val_loss_history = []\n",
    "    train_acc_history = []\n",
    "    train_loss_history = []\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "    \n",
    "    # initialize the early_stopping object\n",
    "    early_stopping = EarlyStopping(patience=patience, verbose=True)\n",
    "\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('\\nEpoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:               \n",
    "                model.eval()   # Set model to evaluate mode\n",
    "                #update_bn_stats(model=model, data_loader=dataloaders[phase])  # if update_bn_stats\n",
    "                \n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    # Get model outputs and calculate loss\n",
    "                    # Special case for inception because in training it has an auxiliary output. In train\n",
    "                    #   mode we calculate the loss by summing the final output and the auxiliary output\n",
    "                    #   but in testing we only consider the final output.\n",
    "                    \n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                      # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "                \n",
    "                if phase == 'train':\n",
    "                    train_f1score = f1_score(labels.cpu().data, preds.cpu(), average='macro')\n",
    "                elif phase == 'val':\n",
    "                    val_f1score = f1_score(labels.cpu().data, preds.cpu(), average='macro')\n",
    "                    \n",
    "\n",
    "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
    "            \n",
    "            print('{} Loss: {:.4f} Acc: {:.4f} f1_score: {:.4f}'.format(phase, epoch_loss, epoch_acc, eval(phase + '_f1score'))\n",
    "            \n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            if phase == 'val':\n",
    "                val_acc_history.append(epoch_acc)\n",
    "                val_loss_history.append(epoch_loss)\n",
    "                \n",
    "#                 # early_stopping needs the validation loss to check if it has decresed, \n",
    "#                 # and if it has, it will make a checkpoint of the current model\n",
    "#                 early_stopping(epoch_loss, model)\n",
    "#                 if early_stopping.early_stop:\n",
    "#                     best_acc = epoch_acc\n",
    "#                     best_model_wts = copy.deepcopy(model.state_dict())\n",
    "#                     print(\"Early stopping\")\n",
    "#                     break     \n",
    "            if phase == 'train':\n",
    "                train_acc_history.append(epoch_acc)\n",
    "                train_loss_history.append(epoch_loss)\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "   \n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, val_acc_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18976e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, dl, normalize=True):\n",
    "    model.eval()\n",
    "    true_labels = []\n",
    "    predictions = []\n",
    "    total = 0\n",
    "    num_correct = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in dl:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            predicted = torch.argmax(outputs.data,-1)        \n",
    "            true_labels.append(labels.cpu().numpy())\n",
    "            predictions.append(predicted.cpu().numpy())\n",
    "            total += labels.size(0)\n",
    "            num_correct += (predicted == labels).sum()\n",
    "        print(f'Test Accuracy of the model: {float(num_correct)/float(total)*100:.2f}')    \n",
    "        true_labels = np.hstack(true_labels)\n",
    "        predictions = np.hstack(predictions)\n",
    "\n",
    "    return true_labels, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2672e78d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x, y - find the img from class x labelled as class y \n",
    "def test(model, dl, x, y, normalize=True):\n",
    "    model.eval()\n",
    "    true_labels = []\n",
    "    predictions = []\n",
    "    images_list = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in dl:\n",
    "            images_list.append(images.cpu().numpy())\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            predicted = torch.argmax(outputs.data,-1)        \n",
    "            true_labels.append(labels.cpu().numpy())\n",
    "            predictions.append(predicted.cpu().numpy())\n",
    "    \n",
    "    for n in range(60):\n",
    "        for i in range(32):\n",
    "            if (true_labels[n][i] == x)  & (predictions[n][i] == y):\n",
    "                #inv_tensor = inv_normalize(image_list[n][i]])\n",
    "                plt.imshow(np.transpose(images_list[n][i], (1, 2, 0)))\n",
    "                plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04277517",
   "metadata": {},
   "source": [
    "# Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39bdcdc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_list = [1, 2]\n",
    "learning_rate_list = [0.000559, 0.0001]\n",
    "batch_size_list = [64]\n",
    "optimizer_list = [\n",
    "    'Adam', \n",
    "    'AdamW', \n",
    "#     'SGD'\n",
    "]\n",
    "\n",
    "model_list = [\n",
    "    'resnet',\n",
    "#     'densenet'\n",
    "]\n",
    "\n",
    "grid_list = [\n",
    "    epochs_list,\n",
    "    learning_rate_list,\n",
    "    batch_size_list,\n",
    "    optimizer_list,\n",
    "    model_list\n",
    "            ]\n",
    "\n",
    "grid = list(itertools.product(*grid_list))\n",
    "random.shuffle(grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f4980b",
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_mean = (0.676, 0.542, 0.519)\n",
    "norm_std = (0.290, 0.226, 0.237)\n",
    "num_workers = 16\n",
    "feature_extract=True\n",
    "num_loop = 2\n",
    "\n",
    "results_df = pd.DataFrame(columns=[\n",
    "    'dataset',\n",
    "    'num_classes',\n",
    "    'model',\n",
    "    'val_accuracy',\n",
    "    'train_time (min)',\n",
    "    'epochs',\n",
    "    'learning_rate',\n",
    "    'optimizer',\n",
    "    'batch_size',\n",
    "])\n",
    "\n",
    "for i in range(num_loop):\n",
    "    print(f'''\n",
    "  {i+1}. epochs={grid[i][0]} \n",
    "     learning_rate={grid[i][1]} \n",
    "     batch_size={grid[i][2]} \n",
    "     optimizer={grid[i][3]}\n",
    "     model={grid[i][4]}\n",
    "        ''') \n",
    "    start_time = time.time()#datetime.now(timezone('America/Chicago')).strftime('%Y%m%d-%H%M%S')\n",
    "    model_ft, input_size = initialize_model(model_name=grid[i][4], num_classes=len(classes), \n",
    "                                            feature_extract=feature_extract)\n",
    "    \n",
    "    train_dataset =ImageFolder(root = DIR_TRAIN, transform=transforms.Compose([\n",
    "        transforms.Resize((input_size, input_size)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomVerticalFlip(),\n",
    "        transforms.RandomRotation(20),\n",
    "        transforms.RandomCrop(size=(input_size, input_size)),\n",
    "        transforms.ToTensor(), transforms.Normalize(norm_mean, norm_std)]))\n",
    "\n",
    "    valid_dataset = ImageFolder(root = DIR_VAL, transform=transforms.Compose([\n",
    "        transforms.Resize((input_size, input_size)), \n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(norm_mean, norm_std)]))\n",
    "\n",
    "    test_dataset = ImageFolder(root = DIR_TEST, transform=transforms.Compose([\n",
    "        transforms.Resize((input_size,input_size)), \n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(norm_mean, norm_std)]))\n",
    "    \n",
    "    dataloaders_dict = {}\n",
    "    dataloaders_dict['train'] = DataLoader(train_dataset, batch_size=grid[i][2], \n",
    "                                           shuffle=True, num_workers=num_workers)\n",
    "    dataloaders_dict['val'] = DataLoader(valid_dataset, batch_size=grid[i][2], \n",
    "                                         shuffle=False, num_workers=num_workers, drop_last=False)\n",
    "    dataloader_test = DataLoader(test_dataset, batch_size=grid[i][2], shuffle=False, \n",
    "                                 num_workers=num_workers, drop_last=False)\n",
    "    \n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    model_ft = model_ft.to(device)\n",
    "    \n",
    "    # Gather the parameters to be optimized/updated in this run. If we are\n",
    "    #  fine tuning we will be updating all parameters. However, if we are \n",
    "    #  doing feature extract method, we will only update the parameters\n",
    "    #  that we have just initialized, i.e. the parameters with requires_grad\n",
    "    #  is True.\n",
    "    params_to_update = model_ft.parameters()\n",
    "    print(\"Params to learn:\")\n",
    "    if feature_extract:\n",
    "        params_to_update = []\n",
    "        for name,param in model_ft.named_parameters():\n",
    "            if param.requires_grad == True:\n",
    "                params_to_update.append(param)\n",
    "                print(\"\\t\",name)\n",
    "    else:\n",
    "        for name,param in model_ft.named_parameters():\n",
    "            if param.requires_grad == True:\n",
    "                print(\"\\t\",name)\n",
    "                             \n",
    "    optimizer_ft = getattr(torch.optim, grid[i][3])(model_ft.parameters(), lr=grid[i][1])\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    model_ft, hist = train_model(model_ft, dataloaders_dict, criterion, optimizer_ft, \n",
    "                                 num_epochs=grid[i][0], patience=3)\n",
    "    \n",
    "    end_time = time.time() - start_time\n",
    "    \n",
    "    row = {\n",
    "        'dataset': data_split,\n",
    "        'num_classes': int(len(classes)),\n",
    "        'model': grid[i][4],\n",
    "        'val_accuracy': f'{hist[0].item():.2f}',\n",
    "        'train_time (min)': f'{end_time/60:.0f}',\n",
    "        'epochs': int(grid[i][0]),\n",
    "        'learning_rate': grid[i][1],\n",
    "        'optimizer': grid[i][3],\n",
    "        'batch_size': int(grid[i][2])\n",
    "    }\n",
    "    \n",
    "    new_df = pd.DataFrame([row])\n",
    "    results_df = pd.concat([results_df, new_df], axis=0, ignore_index=True).sort_values('val_accuracy', ascending = False)\n",
    "\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e666dfed",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = f'{data_split}_resnet50'\n",
    "torch.save(model, f'model/{model_name}.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae5e72b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load(f'model/{model_name}.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b89c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_labels, predictions = test_model(model, dataloader_test, normalize=True)\n",
    "c_matrix = confusion_matrix(true_labels, predictions, normalize='true')\n",
    "plt.figure(figsize=(12, 12))\n",
    "plt.title(\"Confusion matrix\")\n",
    "sns.heatmap(c_matrix, cmap='Blues', annot=True, xticklabels=classes, yticklabels=classes, fmt='.1%', cbar=True)\n",
    "plt.xlabel('predictions')\n",
    "plt.ylabel('true labels')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39520256",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # False prediction\n",
    "# test(model, dataloader_test, 2, 0) #(potentially malignant skin tumors, non-cancerous skin condition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0057e35e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Correct prediction of autoimmune disorder\n",
    "# test(model, dataloader_test, 3, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca4e684",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Trace model\n",
    "\n",
    "# # must be the same size a minibatch with 1 example image\n",
    "# example = torch.rand(1, 3, 224, 224)\n",
    "\n",
    "# # move model back to cpu, do tracing, and optimize\n",
    "# model_conv = model.to('cpu')\n",
    "# traced_script_module = torch.jit.trace(model_conv, example)\n",
    "# torchscript_model_optimized = optimize_for_mobile(traced_script_module)\n",
    "\n",
    "# # save optimized model for mobile\n",
    "# PATH = f'model/{model_name}_traced.pt'\n",
    "# torchscript_model_optimized.save(PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f64a33c",
   "metadata": {},
   "source": [
    "# Conversion from PyTorch to CoreML\n",
    "https://github.com/vincentfpgarcia/from-pytorch-to-coreml/blob/master/step6_part1.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f1b05e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import coremltools as ct\n",
    "from coremltools.converters import ClassifierConfig\n",
    "\n",
    "classifier_config = ClassifierConfig(class_labels=class_list)\n",
    "\n",
    "model = torch.load(f'model/{model_name}.pt')\n",
    "\n",
    "# Create dummy input\n",
    "dummy_input = torch.rand(1, 3, 224, 224)\n",
    "\n",
    "model_conv = model.to('cpu')\n",
    "\n",
    "# Trace the model\n",
    "traced_model = torch.jit.trace(model_conv, dummy_input)\n",
    "\n",
    "# Create the input image type\n",
    "input_image = ct.ImageType(name=\"my_input\", shape=(1, 3, 224, 224), scale=1/255)\n",
    "\n",
    "# Convert the model\n",
    "coreml_model = ct.convert(traced_model, inputs=[input_image], classifier_config=classifier_config)\n",
    "\n",
    "# Modify the output's name to \"my_output\" in the spec\n",
    "spec = coreml_model.get_spec()\n",
    "ct.utils.rename_feature(spec, \"var_840\", \"my_output\")\n",
    "\n",
    "# Re-create the model from the updated spec\n",
    "coreml_model_updated = ct.models.MLModel(spec)\n",
    "\n",
    "# Save the CoreML model\n",
    "coremlmodel_name = 'skindiseases5'\n",
    "coreml_model_updated.save(f'model/{coremlmodel_name}.mlmodel')\n",
    "\n",
    "# Load the CoreML model\n",
    "model =  ct.models.MLModel(f'model/{coremlmodel_name}.mlmodel')\n",
    "\n",
    "# Display its specifications\n",
    "print()\n",
    "print(model)\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "# classes\n",
    "classes = ['Benign Marking or Mole',\n",
    "           'Non-Cancerous Skin Condition',\n",
    "           'Potentially Malignant Skin Tumors', \n",
    "           'Toxin, Fungal, Bug, Viral, or Bacterial Infections',\n",
    "           'Unclassified']\n",
    "\n",
    "# Load the test image\n",
    "image = Image.open('inference/Potentially Malignant Skin Tumors/melanoma.jpeg')\n",
    "\n",
    "# Prediction vector as a numpy array\n",
    "pred = model.predict({'my_input': image.resize((224, 224))})\n",
    "pred = pred['my_output']\n",
    "pred = pred.squeeze()\n",
    "\n",
    "# Display the most probable class\n",
    "idx = pred.argmax()\n",
    "print('Predicted class : %d (%s)' % (idx, classes[idx]))\n",
    "pred['classLabel']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "210f123d",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbdfcb8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_dataset = ImageFolder(root = 'inference/', transform=transforms.Compose([\n",
    "    transforms.Resize(255),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.676, 0.542, 0.519], std=[0.290, 0.226, 0.237])\n",
    "]))\n",
    "\n",
    "dataloader_inference = DataLoader(inference_dataset, batch_size=1, \n",
    "                                  shuffle=False, num_workers=1, drop_last=False)\n",
    "\n",
    "def inference_model(model, dl, normalize=True):\n",
    "    model.eval()\n",
    "    true_labels = []\n",
    "    predictions = []\n",
    "    total = 0\n",
    "    num_correct = 0\n",
    "    # device = torch.device('cpu')\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in dl:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            predicted = torch.argmax(outputs.data,-1)        \n",
    "            true_labels.append(labels.cpu().numpy())\n",
    "            predictions.append(predicted.cpu().numpy())\n",
    "            total += labels.size(0)\n",
    "            num_correct += (predicted == labels).sum()\n",
    "        print(f'Inference Accuracy of the model: {float(num_correct)/float(total)*100:.2f}')    \n",
    "        true_labels = np.hstack(true_labels)\n",
    "        predictions = np.hstack(predictions)\n",
    "\n",
    "    return true_labels, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46768b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = torch.jit.load(f'model/{model_name}_traced.pt')\n",
    "model = torch.load(f'model/{model_name}.pt')\n",
    "true_labels, predictions = inference_model(model, dataloader_inference, normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77975991",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb6c13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c624a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !tar -czf model/model.tar.gz model/merged_resnet50.pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46523244",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !aws s3 cp model/model.tar.gz s3://rubyhan-w210-datasets/model/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "w210env",
   "language": "python",
   "name": "w210env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
