{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nB6QjlVmr01i"
   },
   "source": [
    "# Modeling - First Pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iVEzW45qsn4t"
   },
   "source": [
    "### Installs, Packages, Seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rUgahMpfsKKF",
    "outputId": "2888a465-c4a0-4425-ae4f-dd9b9e0694c0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Requirement already satisfied: efficientnet_pytorch in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (0.7.1)\n",
      "Requirement already satisfied: torch in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from efficientnet_pytorch) (1.10.0)\n",
      "Requirement already satisfied: typing_extensions in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from torch->efficientnet_pytorch) (4.0.0)\n",
      "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 22.3.1 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/pytorch_p38/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install efficientnet_pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Requirement already satisfied: torch in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (1.10.0)\n",
      "Requirement already satisfied: typing_extensions in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from torch) (4.0.0)\n",
      "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 22.3.1 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/pytorch_p38/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install torch\n",
    "# %pip install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "hGcQYrTsrhNM"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# python libraties\n",
    "import os\n",
    "import cv2\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import seaborn as sns\n",
    "from itertools import combinations, product\n",
    "\n",
    "# import imblearn\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "from glob import glob\n",
    "from PIL import Image\n",
    "import ipywidgets\n",
    "\n",
    "# pytorch libraries\n",
    "import torch\n",
    "from torch import optim,nn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "from torchvision import models,transforms\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "\n",
    "# sklearn libraries\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# google drive\n",
    "# from google.colab import drive # Connect colab to google drive\n",
    "\n",
    "# custom modeling libraries\n",
    "from build_model2 import initialize_model, load_split_data, build_loader, evaluate, train_model, model_scores, eval_model, add_results\n",
    "\n",
    "# other\n",
    "import importlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import build_model2\n",
    "importlib.reload(build_model2)\n",
    "\n",
    "from build_model2 import initialize_model, load_split_data, build_loader, evaluate, train_model, model_scores, eval_model, add_results\n",
    "\n",
    "# import matplotlib as mpl\n",
    "# print(mpl.get_cachedir())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# epoch_search = [50]\n",
    "# # optim_search = ['SGD']\n",
    "# model_search = ['resnet', 'vgg']\n",
    "# lr_search = [.003, .0035]\n",
    "# split_search = ['split_8', 'split_9', 'split_10']\n",
    "\n",
    "# prods = list(product(epoch_search, model_search, lr_search, split_search))\n",
    "\n",
    "# es = pd.Series(list(zip(*prods))[0], name = 'epochs', dtype = 'int')\n",
    "# mods = pd.Series(list(zip(*prods))[1], name = 'pretrained_model')\n",
    "# learns = pd.Series(list(zip(*prods))[2], name = 'learning_rate')\n",
    "# splits = pd.Series(list(zip(*prods))[3], name = 'data_split')\n",
    "\n",
    "# g_search = pd.concat([es, mods, learns, splits], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load custom grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epochs</th>\n",
       "      <th>pretrained_model</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>data_split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>50</td>\n",
       "      <td>resnet</td>\n",
       "      <td>0.003</td>\n",
       "      <td>split_8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50</td>\n",
       "      <td>resnet</td>\n",
       "      <td>0.003</td>\n",
       "      <td>split_9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>50</td>\n",
       "      <td>resnet</td>\n",
       "      <td>0.003</td>\n",
       "      <td>split_10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>50</td>\n",
       "      <td>vgg</td>\n",
       "      <td>0.003</td>\n",
       "      <td>split_8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>50</td>\n",
       "      <td>vgg</td>\n",
       "      <td>0.003</td>\n",
       "      <td>split_9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>50</td>\n",
       "      <td>vgg</td>\n",
       "      <td>0.003</td>\n",
       "      <td>split_10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   epochs pretrained_model  learning_rate data_split\n",
       "0      50           resnet          0.003    split_8\n",
       "1      50           resnet          0.003    split_9\n",
       "2      50           resnet          0.003   split_10\n",
       "3      50              vgg          0.003    split_8\n",
       "4      50              vgg          0.003    split_9\n",
       "5      50              vgg          0.003   split_10"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_grid = pd.read_csv('/home/ec2-user/SageMaker/teledermatologyAI_capstone/model/gridsearch5/custom_grid.csv')\n",
    "g_search = custom_grid\n",
    "g_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_search.index+=500 # index to add to prior run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "576JOVVyzMBp"
   },
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Type: Tesla T4\n",
      "GPU Count: 1\n"
     ]
    }
   ],
   "source": [
    "model_dict = {'pretrained_model': None, \n",
    "              'epochs': None, # NEEDS UPDATE\n",
    "              'home_directory': '/home/ec2-user/SageMaker/teledermatologyAI_capstone',\n",
    "              'mod_directory': '/home/ec2-user/SageMaker/teledermatologyAI_capstone/model/gridsearch5',\n",
    "              'csv_name': 'full_data_final_diverse',\n",
    "              'split': 'split_3',\n",
    "              'cl': 'label_0',\n",
    "              'dev_state': False,\n",
    "              'dev_sample': 15000,\n",
    "              'seed': 99,\n",
    "              'lr': .0035,                  # from prior gridsearch\n",
    "              'batch_size':64,\n",
    "              'num_workers':24,\n",
    "              'transform':3,\n",
    "              'results_file':'gridsearch_results',\n",
    "              'model':None, # NEEDS UPDATE\n",
    "              'device': torch.device('cuda:0'), # NEEDS UPDATE\n",
    "              'optimizer': None, # NEEDS UPDATE\n",
    "              'criterion': None, # NEEDS UPDATE\n",
    "              'tuned_model_name': None, # NEEDS UPDATE\n",
    "              'show_val_cm': False,\n",
    "             }\n",
    "\n",
    "np.random.seed(model_dict['seed'])\n",
    "torch.cuda.manual_seed(model_dict['seed'])\n",
    "\n",
    "# Check GPU\n",
    "print('GPU Type:', torch.cuda.get_device_name())\n",
    "print('GPU Count:', torch.cuda.device_count())\n",
    "\n",
    "HOME = model_dict['home_directory']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/SageMaker/teledermatologyAI_capstone\n"
     ]
    }
   ],
   "source": [
    "cd $HOME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2TaPtH1nqVCm"
   },
   "source": [
    "## In for loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Rate: 0.003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_16700/3062223369.py:23: DtypeWarning: Columns (7,14,15,16,17,18,19,20,21,22,23,24) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  data, train, test, val = load_split_data(directory = model_dict['home_directory'],\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training resnet_50e_SGD_split_8_diverse_GS500\n",
      "[epoch 1], [iter 100 of 283],[train loss 1.27063], [train acc 0.49578]\n",
      "[epoch 1], [iter 200 of 283],[train loss 1.12201], [train acc 0.54977]\n",
      "------------------------------------------------------------\n",
      "[epoch 1], [val loss 0.82244], [val acc 0.65832]\n",
      "------------------------------------------------------------\n",
      "\n",
      "EPOCH 1 :\n",
      "*****************************************************\n",
      "Complete in 3m 16s\n",
      "best record: [epoch 1], [val loss 0.82244], [val acc 0.65832]\n",
      "*****************************************************\n",
      "[epoch 2], [iter 100 of 283],[train loss 0.83903], [train acc 0.65766]\n",
      "[epoch 2], [iter 200 of 283],[train loss 0.82795], [train acc 0.66266]\n",
      "------------------------------------------------------------\n",
      "[epoch 2], [val loss 0.75270], [val acc 0.69196]\n",
      "------------------------------------------------------------\n",
      "\n",
      "EPOCH 2 :\n",
      "*****************************************************\n",
      "Complete in 3m 34s\n",
      "best record: [epoch 2], [val loss 0.75270], [val acc 0.69196]\n",
      "*****************************************************\n",
      "[epoch 3], [iter 100 of 283],[train loss 0.76390], [train acc 0.68594]\n",
      "[epoch 3], [iter 200 of 283],[train loss 0.75932], [train acc 0.68961]\n",
      "------------------------------------------------------------\n",
      "[epoch 3], [val loss 0.70862], [val acc 0.71732]\n",
      "------------------------------------------------------------\n",
      "\n",
      "EPOCH 3 :\n",
      "*****************************************************\n",
      "Complete in 3m 39s\n",
      "best record: [epoch 3], [val loss 0.70862], [val acc 0.71732]\n",
      "*****************************************************\n",
      "[epoch 4], [iter 100 of 283],[train loss 0.72318], [train acc 0.71234]\n",
      "[epoch 4], [iter 200 of 283],[train loss 0.71943], [train acc 0.71203]\n",
      "[epoch 16], [iter 100 of 283],[train loss 0.59177], [train acc 0.76922]\n",
      "[epoch 16], [iter 200 of 283],[train loss 0.59309], [train acc 0.76883]\n",
      "------------------------------------------------------------\n",
      "[epoch 16], [val loss 0.61622], [val acc 0.76810]\n",
      "------------------------------------------------------------\n",
      "\n",
      "EPOCH 16 :\n",
      "*****************************************************\n",
      "Complete in 3m 8s\n",
      "best record: [epoch 16], [val loss 0.61622], [val acc 0.76810]\n",
      "*****************************************************\n",
      "[epoch 17], [iter 100 of 283],[train loss 0.57310], [train acc 0.77734]\n",
      "[epoch 17], [iter 200 of 283],[train loss 0.57710], [train acc 0.77648]\n",
      "------------------------------------------------------------\n",
      "[epoch 17], [val loss 0.60419], [val acc 0.76782]\n",
      "------------------------------------------------------------\n",
      "\n",
      "EPOCH 17 :\n",
      "*****************************************************\n",
      "Complete in 3m 7s\n",
      "best record: [epoch 17], [val loss 0.60419], [val acc 0.76782]\n",
      "*****************************************************\n",
      "[epoch 18], [iter 100 of 283],[train loss 0.58677], [train acc 0.77063]\n",
      "[epoch 18], [iter 200 of 283],[train loss 0.58604], [train acc 0.76969]\n",
      "------------------------------------------------------------\n",
      "[epoch 18], [val loss 0.60289], [val acc 0.77196]\n",
      "------------------------------------------------------------\n",
      "\n",
      "EPOCH 18 :\n",
      "*****************************************************\n",
      "Complete in 3m 10s\n",
      "best record: [epoch 18], [val loss 0.60289], [val acc 0.77196]\n",
      "*****************************************************\n",
      "[epoch 19], [iter 100 of 283],[train loss 0.58337], [train acc 0.77172]\n",
      "[epoch 19], [iter 200 of 283],[train loss 0.57538], [train acc 0.77734]\n",
      "------------------------------------------------------------\n",
      "[epoch 19], [val loss 0.60439], [val acc 0.77228]\n",
      "------------------------------------------------------------\n",
      "\n",
      "EPOCH 19 :\n",
      "*****************************************************\n",
      "Complete in 3m 8s\n",
      "best record: [epoch 19], [val loss 0.60439], [val acc 0.77228]\n",
      "*****************************************************\n",
      "[epoch 20], [iter 100 of 283],[train loss 0.57283], [train acc 0.77750]\n",
      "[epoch 20], [iter 200 of 283],[train loss 0.55479], [train acc 0.78391]\n",
      "------------------------------------------------------------\n",
      "[epoch 20], [val loss 0.62409], [val acc 0.76419]\n",
      "------------------------------------------------------------\n",
      "\n",
      "EPOCH 20 :\n",
      "*****************************************************\n",
      "Complete in 3m 6s\n",
      "best record: [epoch 20], [val loss 0.62409], [val acc 0.76419]\n",
      "*****************************************************\n",
      "[epoch 21], [iter 100 of 283],[train loss 0.55371], [train acc 0.77828]\n",
      "[epoch 21], [iter 200 of 283],[train loss 0.55060], [train acc 0.78273]\n",
      "------------------------------------------------------------\n",
      "[epoch 21], [val loss 0.60644], [val acc 0.77200]\n",
      "------------------------------------------------------------\n",
      "\n",
      "EPOCH 21 :\n",
      "*****************************************************\n",
      "Complete in 3m 6s\n",
      "best record: [epoch 21], [val loss 0.60644], [val acc 0.77200]\n",
      "*****************************************************\n",
      "[epoch 22], [iter 100 of 283],[train loss 0.54036], [train acc 0.78734]\n",
      "[epoch 22], [iter 200 of 283],[train loss 0.54793], [train acc 0.78320]\n"
     ]
    }
   ],
   "source": [
    "# Gridsearch\n",
    "\n",
    "for i in g_search.iterrows():\n",
    "\n",
    "    # extract gridsearch features\n",
    "    model_dict['epochs'] = i[1]['epochs']\n",
    "    model_dict['pretrained_model'] = i[1]['pretrained_model']\n",
    "    model_dict['optimizer_name'] = 'SGD'\n",
    "    model_dict['lr'] = i[1]['learning_rate']\n",
    "    model_dict['split'] = i[1]['data_split']\n",
    "    me = i[1]['epochs']\n",
    "    mn = i[1]['pretrained_model']\n",
    "    mo = model_dict['optimizer_name']\n",
    "    ms = model_dict['split']\n",
    "    mlr = i[1]['learning_rate']\n",
    "    model_dict['alias'] = i[0]\n",
    "    model_dict['tuned_model_name'] = f'{mn}_{me}e_{mo}_{ms}_diverse_GS{i[0]}'\n",
    "    direc = model_dict['mod_directory']\n",
    "    nam = model_dict['tuned_model_name']\n",
    "    print(f'Learning Rate: {mlr}')\n",
    "    \n",
    "    # data for each split\n",
    "    data, train, test, val = load_split_data(directory = model_dict['home_directory'],\n",
    "                                             csv_name = model_dict['csv_name'], \n",
    "                                             data_split = model_dict['split'], \n",
    "                                             label = model_dict['cl'],\n",
    "                                             mode = 'all',\n",
    "                                             dev_state = model_dict['dev_state'], \n",
    "                                             dev_sample = model_dict['dev_sample'], \n",
    "                                             seed = model_dict['seed']\n",
    "                                             )\n",
    "    \n",
    "    # Label dictionary for evaluation\n",
    "    labels_idx = np.sort(data.label_idx.unique())\n",
    "    label_map = data[['label', 'label_idx']].drop_duplicates().sort_values('label_idx')\n",
    "    label_dict = dict(zip(label_map.label_idx, label_map['label']))\n",
    "    model_dict['label_dict'] = label_dict\n",
    "    \n",
    "    # set batch size\n",
    "    if model_dict['pretrained_model'] == 'efficientnet':\n",
    "        model_dict['batch_size'] = 3\n",
    "    else: \n",
    "        model_dict['batch_size'] = 64\n",
    "    \n",
    "    # Load each model\n",
    "    model_ft, input_size = initialize_model(model_name = model_dict['pretrained_model'], \n",
    "                                            num_classes = len(data.label.unique()),\n",
    "                                            feature_extract = False, \n",
    "                                            use_pretrained=True)\n",
    "    \n",
    "    # Move model to GPU\n",
    "    model = model_ft.to(model_dict['device'])\n",
    "    \n",
    "    model_dict.update({\n",
    "                       'model':model,\n",
    "                       'criterion': nn.CrossEntropyLoss().to(model_dict['device']),\n",
    "    })\n",
    "    \n",
    "    # Define optimizer options:\n",
    "    if model_dict['optimizer_name'] == 'SGD':\n",
    "        model_dict.update({'optimizer': optim.SGD(model.parameters(), lr=model_dict['lr'])})\n",
    "    elif model_dict['optimizer_name'] == 'Adam':\n",
    "        model_dict.update({'optimizer': optim.Adam(model.parameters(), lr=model_dict['lr'])})\n",
    "    elif model_dict['optimizer_name'] == 'AdamW':\n",
    "        model_dict.update({'optimizer': optim.AdamW(model.parameters(), lr=model_dict['lr'])})\n",
    "    \n",
    "    # Update dictionary\n",
    "    model_dict['resize'] = int(input_size/.85)\n",
    "\n",
    "    \n",
    "    # Set Transforms\n",
    "    transform_header = [\n",
    "                        transforms.Resize(model_dict['resize']), #255\n",
    "                        transforms.CenterCrop(input_size)\n",
    "                        ]\n",
    "\n",
    "    transform_body = [\n",
    "                      transforms.RandomHorizontalFlip(), # a\n",
    "                      transforms.RandomVerticalFlip(), # b\n",
    "                      transforms.RandomRotation(20), # c\n",
    "                      transforms.RandomCrop(size=(input_size,input_size)), # d\n",
    "#                       transforms.RandomInvert(), transforms.RandomPosterize(bits=2), # e\n",
    "#                       transforms.RandomAdjustSharpness(sharpness_factor=2), # f\n",
    "#                       transforms.RandomSolarize(threshold=192.0), # g\n",
    "#                       transforms.ColorJitter(brightness=0.1, contrast=0.1, hue=0.1) # h\n",
    "                      ]\n",
    "\n",
    "    transform_footer = [transforms.ToTensor(), \n",
    "                      transforms.Normalize(mean=[.541, .414, .382], std=[.256,.215,.209])]\n",
    "    \n",
    "    val_transform = transforms.Compose([\n",
    "                                      transforms.Resize(model_dict['resize']),\n",
    "                                      transforms.CenterCrop(input_size),\n",
    "                                      transforms.ToTensor(), \n",
    "                                      transforms.Normalize(mean=[.541, .414, .382], std=[.256,.215,.209])\n",
    "                                    ])\n",
    "    \n",
    "    test_loader = build_loader(mode = 'test', df = test, transform = val_transform, batch_size = model_dict['batch_size'], num_workers = model_dict['num_workers'])\n",
    "    val_loader = build_loader(mode = 'val', df = val, transform = val_transform, batch_size = model_dict['batch_size'], num_workers = model_dict['num_workers'])   \n",
    "    \n",
    "    transform_list = transform_header + transform_body + transform_footer\n",
    "    train_transform = transforms.Compose(transform_list)\n",
    "    train_loader = build_loader(mode = 'train', df = train, transform = train_transform, batch_size = model_dict['batch_size'], num_workers = model_dict['num_workers'])\n",
    "\n",
    "\n",
    "    loaders = {'train_loader':train_loader,\n",
    "                            'val_loader': val_loader,\n",
    "                            'test_loader': test_loader}\n",
    "    model_dict['loader'] = loaders\n",
    "\n",
    "    pred_df, val_scores, tot_time = train_model(model_dict = model_dict)\n",
    "\n",
    "    acc, f1, f2, f5, prec, rec, d_0, d_1, d_2, d_3, d_4 = val_scores\n",
    "    \n",
    "\n",
    "    pred_df.to_pickle(f'{direc}/{nam}_preds.pkl')\n",
    "    \n",
    "    col_dict = {\n",
    "#              'model': pd.Series(dtype = 'int'),\n",
    "#              'file': pd.Series(dtype = 'str'),\n",
    "             'tuned_model': model_dict['tuned_model_name'],\n",
    "             'transform': model_dict['transform'],\n",
    "             'lr': model_dict['lr'],\n",
    "             'pretrained_model': model_dict['pretrained_model'],\n",
    "             'optimizer': model_dict['optimizer_name'],\n",
    "             'epochs': model_dict['epochs'],\n",
    "#              'num_classes': model_dict['num_classes'],\n",
    "             'batch_size': model_dict['batch_size'],\n",
    "             'workers': model_dict['num_workers'],\n",
    "             'train_time': tot_time,\n",
    "             'data_split': model_dict['split'],\n",
    "             'label_set': model_dict['cl'],\n",
    "             'accur': acc,\n",
    "             'F1': f1,\n",
    "             'F0.5': f5,\n",
    "             'F2': f2,\n",
    "             'benign_accur': d_0,\n",
    "             'noncancerous_accur': d_1,\n",
    "             'malignant_accur': d_2,\n",
    "             'infection_accur': d_3,\n",
    "             'unclassified_accur': d_4\n",
    "    }\n",
    "    \n",
    "#     print(tdf.iloc[:i[0]+1][['transform', 'lr', 'accur']])\n",
    "    add_results(model_dict['results_file'], direc, pd.DataFrame(col_dict, index = [i[0]]))\n",
    "    print('xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # \n",
    "# pred_df = evaluate(model_name = 'gridsearch4/vgg_50e_SGD_split_10_GS220', \n",
    "#                    model_source = 'pt', \n",
    "#                    model_dict = model_dict, \n",
    "#                    label_dict = label_dict, \n",
    "#                    show_cm = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new = pd.read_csv('full_data_final_not_diverse.csv', index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new.iloc[0].path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new[new.image_id == 'ISIC_0000000'].path[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ipath = './Data/ISIC_2019/ISIC_2019_Training_Input/ISIC_0000000.jpg'\n",
    "# image = mp_image.imread(ipath)\n",
    "# # imshow(image)\n",
    "# fig, axes = plt.subplots(nrows = 1, ncols = 1, figsize = (15, 15))\n",
    "# axes.imshow(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot.imshow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "WtiFA177sreH",
    "zV8Kq9caqc_m"
   ],
   "machine_shape": "hm",
   "provenance": [],
   "toc_visible": true
  },
  "gpuClass": "premium",
  "kernelspec": {
   "display_name": "conda_pytorch_p38",
   "language": "python",
   "name": "conda_pytorch_p38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
